What is "Context" in LLMs?
Context means:
ğŸ‘‰ Everything youâ€™ve said (or written) so far in a conversation or prompt
It helps the model understand what you mean now based on what came before.

ğŸ”¸ Real-Life Example:
You say:

"What's the capital of India?"

Then later say:

"What about China?"

If there were no context, Iâ€™d be confused.
But with context, I know you're still asking about capitals â€” so I reply:

"The capital of China is Beijing."

âœ… Context helped me understand what â€œWhat about China?â€ means.



ğŸ”¹ How much context can I remember?
Every LLM has a context window, like a memory span. For example:

GPT-3.5 could handle ~4,000 tokens (about 2,500â€“3,000 words)

GPT-4 can go up to 128,000 tokens (that's about 300 pages!)

So I can remember a LOT â€” but if you go beyond my limit, I may "forget" the start.

ğŸ”¹ Why is context important?
It keeps conversations flowing naturally
You donâ€™t need to repeat everything again and again.

It helps with multi-step reasoning
Like solving a math word problem with many steps.

It helps maintain memory within a session
Like when you say â€œmy resumeâ€ or â€œthe reportâ€ â€” I know what you mean.

examples are in screenshots/context_example_1 and 2

To make it work like chatgpt i have added screenshots/CE_3 file